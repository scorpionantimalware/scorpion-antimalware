###################################################################################
#                                                                                 #
# extract_and_clean_api_calls_sequence.py                                         #
#                                                                                 #
###################################################################################
#                                                                                 #
#    Scorpion Anti-malware is a free Open Source AI-powered Anti-malware          #
#    framework for Researchers.                                                   #
#                                                                                 #
#    Copyright (c) 2024-present  (see AUTHORS.md).                                #
#                                                                                 #
#    This program is free software: you can redistribute it and/or modify         #
#    it under the terms of the GNU General Public License as published by         #
#    the Free Software Foundation, either version 3 of the License, or            #
#    (at your option) any later version.                                          #
#                                                                                 #
#    This program is distributed in the hope that it will be useful,              #
#    but WITHOUT ANY WARRANTY; without even the implied warranty of               #
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the                #
#    GNU General Public License for more details.                                 #
#                                                                                 #
#    You should have received a copy of the GNU General Public License            #
#    along with this program.  If not, see <https://www.gnu.org/licenses/>.       #
#                                                                                 #
###################################################################################


import os
import json
import re
import numpy as np
from keras.preprocessing.text import tokenizer_from_json
from keras.preprocessing.sequence import pad_sequences
from collections import Counter


def api_extraction(file_path):
    hash = False
    api_calls = []

    if os.path.exists(file_path):
        with open(file_path, 'r') as file:
            try:
                load_dict = json.load(file)

                if 'behavior' not in load_dict: return hash, api_calls

                hash = load_dict['target']['file']['md5']

                for process in load_dict['behavior']['processes']:
                    if process['calls']:
                        for call in process['calls']:
                            api_calls.append(call['api'])

            except json.JSONDecodeError:
                print(f"Error decoding JSON for file: {file_path}")
            except KeyError as e:
                print(f"Missing key {e} in file: {file_path}")
            except Exception as e:
                print(f"Unexpected error for file {file_path}: {e}")

    return hash, ' '.join(api_calls).lower()


def cleaning_apis(string):
    '''for spaced seperated strings, delets any word that contains special chars as it is not an API'''
    # Define a regex pattern to match special characters
    special_chars_pattern = re.compile(r'[^\w\s_]')
    cleaned_list = [element for element in string.split(' ') if not special_chars_pattern.search(element)]

    results = ' '.join(cleaned_list).strip()
    if len(cleaned_list) == 0:
        return np.nan
    return results


def remove_repeated_sequences(string):

    lst = string.split(' ')
    dropped_indices = set()
    counter = Counter(tuple(lst[i:i+2]) for i in range(len(lst) - 1))

    for i in range(len(lst) - 2, -1, -1):
        sub = tuple(lst[i:i+2])
        if counter[sub] > 1:
            dropped_indices |= {i, i + 1}
            counter[sub] -= 1

    return ' '.join([x for i, x in enumerate(lst) if i not in dropped_indices])


def word_tokenization(tokenizerJson_path, api_sequence_str, maxlen):
    with open(tokenizerJson_path) as f:
        data = json.load(f)
        word_tokenizer = tokenizer_from_json(data)
    tokens = word_tokenizer.texts_to_sequences(api_sequence_str)
    padded = pad_sequences(tokens, padding='pre', truncating='pre', maxlen=maxlen)

    return padded


def ready_to_model(file_path, tokenizerJson_path, maxlen):

    hash, api_seq = api_extraction(file_path)

    if len(api_seq.split(' ')) <= 1:
        return False, np.zeros(shape=(1,maxlen),dtype=np.int32)

    cleaned_api_seq = cleaning_apis(api_seq)

    rev_api_seq = remove_repeated_sequences(cleaned_api_seq)

    return hash, word_tokenization(tokenizerJson_path, [rev_api_seq], maxlen)
